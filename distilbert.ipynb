{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install translate","execution_count":8,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: translate in /opt/conda/lib/python3.6/site-packages (3.5.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from translate) (7.1.1)\nRequirement already satisfied: pre-commit in /opt/conda/lib/python3.6/site-packages (from translate) (2.4.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.6/site-packages (from translate) (4.5.0)\nRequirement already satisfied: tox in /opt/conda/lib/python3.6/site-packages (from translate) (3.15.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from translate) (2.22.0)\nRequirement already satisfied: nodeenv>=0.11.1 in /opt/conda/lib/python3.6/site-packages (from pre-commit->translate) (1.3.5)\nRequirement already satisfied: virtualenv>=20.0.8 in /opt/conda/lib/python3.6/site-packages (from pre-commit->translate) (20.0.20)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from pre-commit->translate) (1.5.0)\nRequirement already satisfied: identify>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from pre-commit->translate) (1.4.15)\nRequirement already satisfied: cfgv>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from pre-commit->translate) (3.1.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from pre-commit->translate) (5.3.1)\nRequirement already satisfied: toml in /opt/conda/lib/python3.6/site-packages (from pre-commit->translate) (0.10.1)\nRequirement already satisfied: importlib-resources; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from pre-commit->translate) (1.5.0)\nRequirement already satisfied: filelock<4,>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from tox->translate) (3.0.12)\nRequirement already satisfied: six<2,>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from tox->translate) (1.14.0)\nRequirement already satisfied: pluggy<1,>=0.12.0 in /opt/conda/lib/python3.6/site-packages (from tox->translate) (0.13.1)\nRequirement already satisfied: packaging>=14 in /opt/conda/lib/python3.6/site-packages (from tox->translate) (20.3)\nRequirement already satisfied: py<2,>=1.4.17 in /opt/conda/lib/python3.6/site-packages (from tox->translate) (1.8.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->translate) (2019.11.28)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->translate) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->translate) (1.24.3)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->translate) (3.0.4)\nRequirement already satisfied: appdirs<2,>=1.4.3 in /opt/conda/lib/python3.6/site-packages (from virtualenv>=20.0.8->pre-commit->translate) (1.4.3)\nRequirement already satisfied: distlib<1,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from virtualenv>=20.0.8->pre-commit->translate) (0.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->translate) (2.2.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=14->tox->translate) (2.4.6)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom translate import Translator\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import backend as K\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import layers\nfrom keras.engine import InputSpec, Layer\nfrom tensorflow.keras.layers import Dropout, Dense, Input, Embedding,concatenate, SpatialDropout1D, GlobalAveragePooling1D, Conv1D, LSTM\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors, BertWordPieceTokenizer","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=240, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_cnn_model(transformer, max_len):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    \n    embed = transformer.weights[0].numpy()\n    embedding = Embedding(np.shape(embed)[0], np.shape(embed)[1],\n                          input_length=max_len, weights=[embed],\n                          trainable=False)(input_word_ids)\n    \n    embedding1 = SpatialDropout1D(0.8)(embedding)\n    \"\"\"\n    conv_1 = Conv1D(16, 2)(embedding1)\n    conv_2 = Conv1D(16, 3)(embedding1)\n    conv_3 = Conv1D(16, 4)(embedding1)\n    conv_4 = Conv1D(16, 5)(embedding1)\n    \n    maxpool_1 = GlobalAveragePooling1D()(conv_1)\n    maxpool_2 = GlobalAveragePooling1D()(conv_2)\n    maxpool_3 = GlobalAveragePooling1D()(conv_3)\n    maxpool_4 = GlobalAveragePooling1D()(conv_4)\n    conc = concatenate([maxpool_1, maxpool_2, maxpool_3, maxpool_4], axis=-1)\n    \n    \n    \"\"\"\n    flat = layers.Flatten()(embedding1)\n    drop = Dropout(0.5)(flat)\n    conc2 = Dense(1, activation='sigmoid')(drop)\n    \n    model = Model(inputs=input_word_ids, outputs=conc2)\n    \n    model.compile(Adam(lr=0.01), \n                  loss='binary_crossentropy', \n                  metrics=[tf.keras.metrics.AUC()])\n    \n    return model","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttentionWeightedAverage(Layer):\n    \"\"\"\n    Computes a weighted average attention mechanism from:\n        Zhou, Peng, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao and Bo Xu.\n        “Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification.”\n        ACL (2016). http://www.aclweb.org/anthology/P16-2034\n    How to use:\n    see: [BLOGPOST]\n    \"\"\"\n\n    def __init__(self, return_attention=False, **kwargs):\n        self.init = initializers.get('uniform')\n        self.supports_masking = True\n        self.return_attention = return_attention\n        super(AttentionWeightedAverage, self).__init__(** kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(ndim=3)]\n        assert len(input_shape) == 3\n\n        self.w = self.add_weight(shape=(input_shape[2], 1),\n                                 name='{}_w'.format(self.name),\n                                 initializer=self.init)\n        self.trainable_weights = [self.w]\n        super(AttentionWeightedAverage, self).build(input_shape)\n\n    def call(self, h, mask=None):\n        h_shape = K.shape(h)\n        d_w, T = h_shape[0], h_shape[1]\n        \n        logits = K.dot(h, self.w)  # w^T h\n        logits = K.reshape(logits, (d_w, T))\n        alpha = K.exp(logits - K.max(logits, axis=-1, keepdims=True))  # exp\n        \n        # masked timesteps have zero weight\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            alpha = alpha * mask\n        alpha = alpha / K.sum(alpha, axis=1, keepdims=True) # softmax\n        r = K.sum(h * K.expand_dims(alpha), axis=1)  # r = h*alpha^T\n        h_star = K.tanh(r)  # h^* = tanh(r)\n        if self.return_attention:\n            return [h_star, alpha]\n        return h_star\n\n    def get_output_shape_for(self, input_shape):\n        return self.compute_output_shape(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[2]\n        if self.return_attention:\n            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n        return (input_shape[0], output_len)\n\n    def compute_mask(self, input, input_mask=None):\n        if isinstance(input_mask, list):\n            return [None] * len(input_mask)\n        else:\n            return None","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_lstm_model(transformer, max_len):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    \n    embed = transformer.weights[0].numpy()\n    embedding = Embedding(np.shape(embed)[0], np.shape(embed)[1],\n                          input_length=max_len, weights=[embed],\n                          trainable=False)(input_word_ids)\n    \n    embedding = SpatialDropout1D(0.3)(embedding)\n    lstm_1 = LSTM(16, return_sequences=True)(embedding)\n    \n    #x,attention = AttentionWeightedAverage(return_attention=True)(lstm_2)\n    flat = layers.Flatten()(lstm_1)\n    drop = Dropout(0.5)(flat)\n    conc = Dense(1, activation='sigmoid')(drop)\n    \n    model = Model(inputs=input_word_ids, outputs=conc)\n    \n    model.compile(Adam(lr=0.001), \n                  loss='binary_crossentropy', \n                  metrics=[\"accuracy\"])\n    \n    return model","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\ntrain2.toxic = train2.toxic.round().astype(int)\nvalid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n\ntrain = pd.concat([\n    train1[['comment_text', 'toxic']].query('toxic==0').sample(n=21384, random_state=0),\n    train1[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=112226, random_state=0),\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nvalid[\"lang\"].replace(\"tr\",\"turkish\",inplace=True)\nvalid[\"lang\"].replace(\"es\",\"spanish\",inplace=True)\nvalid[\"lang\"].replace(\"it\",\"italian\",inplace=True)\nfor i in range(len(valid[\"lang\"])):\n    translator= Translator(from_lang=valid.iloc[i,2],to_lang=\"English\")\n    valid.iloc[i,0] = translator.translate(valid.iloc[i,1])\n    if(i%1000==0):\n        print(i)","execution_count":15,"outputs":[{"output_type":"stream","text":"0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nCPU times: user 46.9 s, sys: 3.52 s, total: 50.4 s\nWall time: 32min 47s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    text = text.fillna(\"fillna\").str.lower()\n    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n    text = text.map(lambda x: re.sub(\"\\(http://.*?\\s\\(http://.*\\)\",'',str(x)))\n    return text\n\nvalid[\"comment_text\"] = clean(valid[\"id\"])\ntrain[\"comment_text\"] = clean(train[\"comment_text\"])","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train1,train2","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\n\nEPOCHS = 1\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n\nsave_path = '/kaggle/working/distilbert_base_uncased/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)","execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81cdba3bee184de1a92b20f32cb93774"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_train = fast_encode(train.comment_text.astype(str).values, \n                      fast_tokenizer, maxlen=512)\nx_valid = fast_encode(valid.comment_text.astype(str).values, \n                      fast_tokenizer, maxlen=512)\n\ny_valid = valid.toxic.values\ny_train = train.toxic.values\n\nn=x_train.shape[0]","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del x_train,y_train","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL='distilbert-base-multilingual-cased'\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model_cnn = build_cnn_model(transformer_layer, max_len=512)\n\nmodel_cnn.summary()","execution_count":37,"outputs":[{"output_type":"stream","text":"Model: \"model_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 512)]             0         \n_________________________________________________________________\nembedding_5 (Embedding)      (None, 512, 768)          91812096  \n_________________________________________________________________\nspatial_dropout1d_5 (Spatial (None, 512, 768)          0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 393216)            0         \n_________________________________________________________________\ndropout_117 (Dropout)        (None, 393216)            0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 393217    \n=================================================================\nTotal params: 92,205,313\nTrainable params: 393,217\nNon-trainable params: 91,812,096\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_history = model_cnn.fit(\n    train_dataset,\n    steps_per_epoch=1000,\n    validation_data=valid_dataset,\n    epochs=1\n)\n","execution_count":39,"outputs":[{"output_type":"stream","text":"Train for 1000 steps, validate for 32 steps\n1000/1000 [==============================] - 30s 30ms/step - loss: 2.0433 - auc_4: 0.9795 - val_loss: 61.4111 - val_auc_4: 0.5000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model_lstm = build_lstm_model(transformer_layer, max_len=512)\n\nmodel_lstm.summary()","execution_count":52,"outputs":[{"output_type":"stream","text":"Model: \"model_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 512)]             0         \n_________________________________________________________________\nembedding_9 (Embedding)      (None, 512, 768)          91812096  \n_________________________________________________________________\nspatial_dropout1d_9 (Spatial (None, 512, 768)          0         \n_________________________________________________________________\nlstm_6 (LSTM)                (None, 512, 16)           50240     \n_________________________________________________________________\nflatten_5 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndropout_195 (Dropout)        (None, 8192)              0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 1)                 8193      \n=================================================================\nTotal params: 91,870,529\nTrainable params: 58,433\nNon-trainable params: 91,812,096\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_history2 = model_lstm.fit(\n    train_dataset,\n    steps_per_epoch=50,\n    validation_data=valid_dataset,\n    epochs=1\n)\n","execution_count":53,"outputs":[{"output_type":"stream","text":"Train for 50 steps, validate for 32 steps\n50/50 [==============================] - 9s 187ms/step - loss: 0.0361 - accuracy: 0.9902 - val_loss: 5.5258 - val_accuracy: 0.8463\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\npre=model_lstm.predict(valid_dataset)\nprint(roc_auc_score(y_valid,pre))","execution_count":54,"outputs":[{"output_type":"stream","text":"0.5\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre.shape","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"(8000, 512, 1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_distilbert_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    cls_token = Dropout(0.2)(cls_token)\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    \n    model.compile(Adam(lr=1.5e-3), \n                  loss='binary_crossentropy', \n                  metrics=[tf.keras.metrics.AUC()])\n    \n    return model","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model_distilbert = build_distilbert_model(transformer_layer, max_len=512)\n\nmodel_distilbert.summary()","execution_count":56,"outputs":[{"output_type":"stream","text":"Model: \"model_8\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 512)]             0         \n_________________________________________________________________\ntf_distil_bert_model_10 (TFD ((None, 512, 768),)       134734080 \n_________________________________________________________________\ntf_op_layer_strided_slice (T [(None, 768)]             0         \n_________________________________________________________________\ndropout_215 (Dropout)        (None, 768)               0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 1)                 769       \n=================================================================\nTotal params: 134,734,849\nTrainable params: 134,734,849\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history3 = model_distilbert.fit(\n    train_dataset,\n    steps_per_epoch=90,\n    validation_data=valid_dataset,\n    epochs=1\n)","execution_count":60,"outputs":[{"output_type":"stream","text":"Train for 80 steps, validate for 32 steps\n80/80 [==============================] - 73s 909ms/step - loss: 0.3573 - auc_5: 0.4373 - val_loss: 1.4935 - val_auc_5: 0.5000\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}