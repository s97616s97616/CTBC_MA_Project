{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\nfrom keras import layers,optimizers\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\ntrain2.toxic = train2.toxic.round().astype(int)\n#valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\nvalid = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_valid_translated.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([\n    train1[['comment_text', 'toxic']].query('toxic==0').sample(n=21384, random_state=0),\n    train1[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=112226, random_state=0),\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n%%time \n\nvalid[\"lang\"].replace(\"tr\",\"turkish\",inplace=True)\nvalid[\"lang\"].replace(\"es\",\"spanish\",inplace=True)\nvalid[\"lang\"].replace(\"it\",\"italian\",inplace=True)\nfor i in range(len(valid[\"lang\"])):\n    translator= Translator(from_lang=valid.iloc[i,2],to_lang=\"English\")\n    valid.iloc[i,0] = translator.translate(valid.iloc[i,1])\n    if(i%1000==0):\n        print(i)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    text = text.fillna(\"fillna\").str.lower()\n    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n    text = text.map(lambda x: re.sub(\"\\(http://.*?\\s\\(http://.*\\)\",'',str(x)))\n    return text\n\nvalid[\"comment_text\"] = clean(valid[\"translated\"])\ntrain[\"comment_text\"] = clean(train[\"comment_text\"])\n\ny_valid = valid.toxic.values\ny_train = train.toxic.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen=1024\ntokenizer = Tokenizer(num_words=maxlen)\n\ntokenizer.fit_on_texts(train[\"comment_text\"])\nxtrain = tokenizer.texts_to_sequences(train[\"comment_text\"])\nxtrain = pad_sequences(xtrain, padding='post', maxlen=maxlen)\n\nxtest = tokenizer.texts_to_sequences(valid[\"comment_text\"])\nxtest = pad_sequences(xtest, padding='post', maxlen=maxlen)\n\nvocab_size = len(tokenizer.word_index)+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmain_input = Input(shape=(maxlen,), dtype='float64')\nembedder = Embedding(input_dim=vocab_size,output_dim=50,input_length=maxlen)\nembed = embedder(main_input)\n\ncnn2 = layers.Conv1D(128, 3, padding='same', strides = 1, activation='relu')(embed)\ncnn2 = layers.MaxPooling1D(pool_size=3)(cnn2)\ncnn3 = layers.Conv1D(128, 4, padding='same', strides = 1, activation='relu')(embed)\ncnn3 = layers.MaxPooling1D(pool_size=3)(cnn3)\ncnn4 = layers.Conv1D(128, 2, padding='same', strides = 1, activation='relu')(embed)\ncnn4 = layers.MaxPooling1D(pool_size=3)(cnn4)\n\ncnn = layers.concatenate([cnn2,cnn3,cnn4], axis=-1)\nflat = layers.Flatten()(cnn)\nmain_output = Dense(1, activation='sigmoid')(flat)\nmodel = Model(inputs = main_input, outputs = main_output)\nmodel.compile(optimizer=\"Adam\",loss='binary_crossentropy',metrics=[tf.keras.metrics.AUC()])\nprint(model.summary())\nhistory = model.fit(xtrain, y_train,epochs=5,verbose=1,validation_data=(xtest, y_valid),batch_size=200)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom keras.layers import SpatialDropout1D\nmain_input = Input(shape=(maxlen,), dtype='float64')\nembedder = Embedding(input_dim=vocab_size,output_dim=100,input_length=maxlen)\nembed = embedder(main_input)\n#embedding = SpatialDropout1D(0.2)(embed)\n#lstm_1 = LSTM(32, return_sequences=True)(embedding)\n\nflat = layers.Flatten()(embed)\n#drop = Dropout(0.1)(flat)\nmain_output = Dense(1, activation='sigmoid')(flat)\nmodel = Model(inputs = main_input, outputs = main_output)\nmodel.compile(optimizer=\"Adam\",loss='binary_crossentropy',metrics=[tf.keras.metrics.AUC()])\nprint(model.summary())\nhistory = model.fit(xtrain, y_train,epochs=5,verbose=1,validation_data=(xtest, y_valid),batch_size=200)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}